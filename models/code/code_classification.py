# #如果huggingface实现就直接复制过来！！！
# #加self.config.use_freezing
# #在models.__init__.py 里面有code_model_classes
# #import 定义名字
# #codet5和t5是不是一个tokenizer
# ## ======== Roberta ========

# #  Vanilla Fine-tuning For Roberta
# class RobertaForCodeClassification(RobertaPreTrainedModel):

# #  Prefix-tuning For BERT
# class RobertaPrefixForCodeClassification(RobertaPreTrainedModel):

# #  Prompt-tuning For RoBERTa
# class RobertaPtuningForCodeClassification(RobertaPreTrainedModel):

# #  Adapter-tuning For RoBERTa
# class RobertaAdapterForCodeClassification(RobertaPreTrainedModel):

# ## ======== CodeBERT ========

# #  Vanilla Fine-tuning For CodeBERT
# class CodeBERTForCodeClassification(CodeBERTPreTrainedModel):

# #  Prefix-tuning For BERT
# class CodeBERTPrefixForCodeClassification(CodeBERTPreTrainedModel):

# #  Prompt-tuning For CodeBERT
# class CodeBERTPtuningForCodeClassification(CodeBERTPreTrainedModel):

# #  Adapter-tuning For CodeBERT
# class CodeBERTAdapterForCodeClassification(CodeBERTPreTrainedModel):

# ## ======== Roberta ========

# #  Vanilla Fine-tuning For Roberta
# class RobertaForCodeClassification(RobertaPreTrainedModel):

# #  Prefix-tuning For BERT
# class RobertaPrefixForCodeClassification(RobertaPreTrainedModel):

# #  Prompt-tuning For RoBERTa
# class RobertaPtuningForCodeClassification(RobertaPreTrainedModel):

# #  Adapter-tuning For RoBERTa
# class RobertaAdapterForCodeClassification(RobertaPreTrainedModel):

# ## ======== GraphCodeBERT ========

# #  Vanilla Fine-tuning For GraphCodeBERT
# class GraphCodeBERTForCodeClassification(GraphCodeBERTPreTrainedModel):

# #  Prefix-tuning For BERT
# class GraphCodeBERTPrefixForCodeClassification(GraphCodeBERTPreTrainedModel):

# #  Prompt-tuning For GraphCodeBERT
# class GraphCodeBERTPtuningForCodeClassification(GraphCodeBERTPreTrainedModel):

# #  Adapter-tuning For GraphCodeBERT
# class GraphCodeBERTAdapterForCodeClassification(GraphCodeBERTPreTrainedModel):

# ## ======== PLBART ========

# #  Vanilla Fine-tuning For PLBART
# class PLBARTForCodeClassification(PLBARTPreTrainedModel):

# #  Prefix-tuning For BERT
# class PLBARTPrefixForCodeClassification(PLBARTPreTrainedModel):

# #  Prompt-tuning For PLBART
# class PLBARTPtuningForCodeClassification(PLBARTPreTrainedModel):

# #  Adapter-tuning For PLBART
# class PLBARTAdapterForCodeClassification(PLBARTPreTrainedModel):

# ## ======== CodeT5 ========

# #  Vanilla Fine-tuning For CodeT5
# class CodeT5ForCodeClassification(CodeT5PreTrainedModel):

# #  Prefix-tuning For BERT
# class CodeT5PrefixForCodeClassification(CodeT5PreTrainedModel):

# #  Prompt-tuning For CodeT5
# class CodeT5PtuningForCodeClassification(CodeT5PreTrainedModel):

# #  Adapter-tuning For CodeT5
# class CodeT5AdapterForCodeClassification(CodeT5PreTrainedModel):
